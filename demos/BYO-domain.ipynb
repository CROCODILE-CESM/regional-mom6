{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring-your-own rotated and/or curvilinear domain \n",
    "\n",
    "This example is forced by GLORYS, ERA5 reanalysis datasets and TPXO tidal model\n",
    "\n",
    "**Note**: FRE-NC tools are required to be set up, as outlined in the [documentation](https://regional-mom6.readthedocs.io/en/latest/) of regional-mom6 package.\n",
    "\n",
    "For this example we need:\n",
    "\n",
    "- [GEBCO bathymetry](https://www.gebco.net/data_and_products/gridded_bathymetry_data/)\n",
    "- [GLORYS ocean reanalysis data](https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/description), and\n",
    "- [ERA5 surface forcing](https://www.ecmwf.int/en/forecasts/dataset/ecmwf-reanalysis-v5)\n",
    "- [TPXO tidel model](https://www.tpxo.net/global)\n",
    "\n",
    "This example reads in the entire global extent of ERA5, GEBCO and TPXO; we don't need to worry about cutting it down to size.\n",
    "This example requires you to bring your own mom-6 compatible model domain input netcdf file called hgrid.nc. This regional mom-6 toolbox can cope with regular, rotated and curvilinear model domains.  \n",
    "\n",
    "Note that this notebook includes some example plots and you will need matplotlib for the plots to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import regional_mom6 as rmom6\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "# from dask.distributed import Client # Use if needed\n",
    "# client = Client()\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Choose our domain, define workspace paths\n",
    "\n",
    "To make sure that things are working I'd recommend starting with a small domain. If this runs ok, then change to the domain of your choice and hopefully it runs ok too! If not, check the [README](https://github.com/COSIMA/regional-mom6/blob/main/README.md) and [documentation](https://regional-mom6.readthedocs.io/) for troubleshooting tips.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_name = \"rotated-demo\"\n",
    "\n",
    "date_range = [\"2003-01-01 00:00:00\", \"2003-01-05 00:00:00\"]\n",
    "\n",
    "## Place where all your input files go \n",
    "input_dir = Path(f\"mom6_input_directories/{expt_name}/\")\n",
    "\n",
    "## Directory where you'll run the experiment from\n",
    "run_dir = Path(f\"mom6_run_directories/{expt_name}/\")\n",
    "\n",
    "## Directory where compiled FRE tools are located (needed for construction of mask tables)\n",
    "toolpath_dir = Path(\"PATH_TO_FRE_TOOLS\")\n",
    "\n",
    "## Path to where your raw ocean forcing files are stored\n",
    "glorys_path = Path(f\"PATH_TO_GLORYS_DATA\" )\n",
    "\n",
    "#Directory where the ERA raw atmospheric output files are stored\n",
    "era_path = Path(\"PATH_TO_ERA5_DATA/era5/single-levels/reanalysis\")\n",
    "#Location of TPXO raw tidal file\n",
    "#note that you will need to swap ## to the version number for each of the file names\n",
    "tide_h_path = Path(\"PATH_TO_TPXO_H_FILE/h_tpxo##.nc\")\n",
    "tide_u_path = Path(\"PATH_TO_TPXO_U_FILE/u_tpxo##.nc\")\n",
    "\n",
    "#location of the BYO hgrid file\n",
    "byogrid_path = \"PATH_TO_YOUR_HORIZONTAL_GRID/small_curvilinear_hgrid.nc\"\n",
    "\n",
    "#location to where your bathymetry data is stored\n",
    "bathy_path = Path(\"PATH_TO_GEBCO_FILE/GEBCO_2022.nc\")\n",
    "\n",
    "## if directories don't exist, create them\n",
    "for path in (run_dir, glorys_path, input_dir):\n",
    "    os.makedirs(str(path), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Copy hgrid.nc into the experinment folder\n",
    "import shutil\n",
    "shutil.copy2(byogrid_path,input_dir/\"hgrid.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Make experiment object\n",
    "The `regional_mom6.experiment` contains the regional domain basics, and also generates the horizontal and vertical grids, `hgrid` and `vgrid` respectively, and sets up the directory structures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt = rmom6.experiment(\n",
    "    date_range = date_range,\n",
    "    resolution = 0.5,\n",
    "    number_vertical_layers = 75,\n",
    "    layer_thickness_ratio = 100,\n",
    "    depth = 4500,\n",
    "    minimum_depth = 5,\n",
    "    mom_run_dir = run_dir,\n",
    "    mom_input_dir = input_dir,\n",
    "    toolpath_dir = toolpath_dir,\n",
    "    hgrid_type = 'from_file',\n",
    "    boundaries = ['north','south','east','west']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now access the horizontal and vertical grid of the regional configuration via `expt.hgrid` and `expt.vgrid` respectively.\n",
    "\n",
    "Plotting the vertical grid with `marker = '.'` lets you see the spacing. You can use `numpy.diff` to compute the vertical spacings, e.g.,\n",
    "```python\n",
    "import numpy as np\n",
    "np.diff(expt.vgrid.zl).plot(marker = '.')\n",
    "```\n",
    "shows you the vertical spacing profile.\n",
    "\n",
    "### Modular workflow!\n",
    "\n",
    "After constructing your `expt` object, if you don't like the default `hgrid` and `vgrid` you can simply modify and then save them back into the `expt` object. However, you'll then also need to save them to disk again. For example:\n",
    "\n",
    "```python\n",
    "new_hgrid = xr.open_dataset(input_dir + \"/hgrid.nc\")\n",
    "```\n",
    "Modify `new_hgrid`, ensuring that _all metadata_ is retained to keep MOM6 happy. Then, save your changes\n",
    "\n",
    "```python\n",
    "expt.hgrid = new_hgrid\n",
    "\n",
    "expt.hgrid.to_netcdf(input_dir + \"/hgrid.nc\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare ocean forcing data\n",
    "\n",
    "We need to cut out our ocean forcing. The package expects an initial condition and one time-dependent segment per non-land boundary. Naming convention is `\"east_unprocessed\"` for segments and `\"ic_unprocessed\"` for the initial condition.\n",
    "\n",
    "In this notebook, we are forcing with the Copernicus Marine \"Glorys\" reanalysis dataset. There's a function in the `mom6-regional` package that generates a bash script to download the correct boundary forcing files for your experiment. First, you will need to create an account with Copernicus, and then call `copernicusmarine login` to set up your login details on your machine. Then you can run the `get_glorys_data.sh` bash script.\n",
    "\n",
    "This bash script uses the [Capernicus marine toolbox]( https://help.marine.copernicus.eu/en/articles/7970514-copernicus-marine-toolbox-installation) and users should install this before running the bash script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.get_glorys(\n",
    "    raw_boundaries_path=glorys_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up bathymetry\n",
    "\n",
    "Similarly to ocean forcing, we point the experiment's `setup_bathymetry` method at the location of the file of choice and also provide the variable names. We don't need to preprocess the bathymetry since it is simply a two-dimensional field and is easier to deal with. Afterwards you can inspect `expt.bathymetry` to have a look at the regional domain.\n",
    "\n",
    "After running this cell, your input directory will contain other bathymetry-related things like the ocean mosaic and mask table too. The mask table defaults to a 10x10 layout and can be modified later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "expt.setup_bathymetry(\n",
    "    bathymetry_path=bathy_path,\n",
    "    longitude_coordinate_name=\"lon\",\n",
    "    latitude_coordinate_name=\"lat\",\n",
    "    vertical_coordinate_name=\"elevation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out your domain: \n",
    "Calling expt.bathymetry returns an xarray dataset, which can be plotted as usual. If you haven't yet run setup_bathymetry, calling expt.bathymetry will return None and prompt you to do so!\n",
    "\n",
    "We do 2 different plots, the first is in x and y coordinates and will look rectangular. The second is in lat and lon coordinates and will show the shape of the curvilinear/rotated domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbval-ignore-output",
     "nbval-skip"
    ]
   },
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "#In x/y coords\n",
    "expt.bathymetry.depth.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "#In lat/lon coords\n",
    "from regional_mom6 import regridding as rgd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "bathy = xr.open_dataset(inputdir/\"bathymetry.nc\")\n",
    "hgrid = xr.open_dataset(inputdir/\"hgrid.nc\")\n",
    "\n",
    "t_points = rgd.get_hgrid_arakawa_c_points(hgrid, point_type = \"t\")\n",
    "\n",
    "plt.pcolormesh(t_points.tlon, t_points.tlat, bathy.depth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 5: Handle the ocean forcing - where the magic happens\n",
    "\n",
    "This cuts out and interpolates the initial condition as well as all boundaries (unless you don't pass it boundaries).\n",
    "\n",
    "The dictionary maps the MOM6 variable names to what they're called in your ocean input file. Notice how for GLORYS, the horizontal dimensions are `latitude` and `longitude`, vs `xh`, `yh`, `xq`, `yq` for MOM6. This is because for an 'A' grid type tracers share the grid with velocities so there's no difference.\n",
    "\n",
    "If one of your segments is land, you can delete its string from the 'boundaries' list. You'll need to update MOM_input to reflect this though so it knows how many segments to look for, and their orientations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping from the GLORYS variables and dimensions to the MOM6 ones\n",
    "ocean_varnames = {\"time\": \"time\",\n",
    "                  \"yh\": \"latitude\",\n",
    "                  \"xh\": \"longitude\",\n",
    "                  \"zl\": \"depth\",\n",
    "                  \"eta\": \"zos\",\n",
    "                  \"u\": \"uo\",\n",
    "                  \"v\": \"vo\",\n",
    "                  \"tracers\": {\"salt\": \"so\", \"temp\": \"thetao\"}\n",
    "                  }\n",
    "\n",
    "# Set up the initial condition\n",
    "expt.setup_initial_condition(\n",
    "    glorys_path / \"ic_unprocessed.nc\", # directory where the unprocessed initial condition is stored, as defined earlier\n",
    "    ocean_varnames,\n",
    "    arakawa_grid=\"A\"\n",
    "    )    \n",
    "\n",
    "# Set up the four boundary conditions. Remember that in the glorys_path, we have four boundary files names north_unprocessed.nc etc. \n",
    "expt.setup_ocean_state_boundaries(\n",
    "        glorys_path,\n",
    "        ocean_varnames,\n",
    "        arakawa_grid = \"A\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out your initial condition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "## Preview the initial temperature at the surface in x/y and lat/lon coordinates\n",
    "from matplotlib import pyplot as plt\n",
    "##Creating a new temperature array with lat and lon coords\n",
    "templatlon = expt.init_tracers.temp.assign_coords(lon=bathy.lon,lat=bathy.lat)\n",
    "\n",
    "#plotting\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(16,4))\n",
    "templatlon.isel(zl=0).plot(ax=axes[0])\n",
    "templatlon.isel(zl=0).plot(x=\"lon\",y=\"lat\",ax=axes[1])\n",
    "axes[0].set_title(\"x/y coords\")\n",
    "axes[1].set_title(\"lon/lat coords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "## u boundary forcing for segment 1 (south)\n",
    "expt.segment_001.u_segment_001.isel(time = 5).plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create Tidal forcing\n",
    "For rectangluar domains (boundaries are aligned with lats and longs) you can use :\n",
    "boundary_type=\"rectangular\"\n",
    "\n",
    "Note that this step can take a while (5 mins +). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "# Tidal files are not available in the repository, and must be requested from the TPXO website\n",
    "expt.setup_boundary_tides(\n",
    "        tide_h_path,\n",
    "        tide_u_path,\n",
    "        tidal_constituents=[\"M2\"],\n",
    "        boundary_type=\"curvilinear\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run the FRE tools\n",
    "\n",
    "This is just a wrapper for the FRE tools needed to make the mosaics and masks for the experiment. The only thing you need to tell it is the processor layout. In this case we're saying that we want a 10 by 10 grid of 100 processors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.run_FRE_tools(layout=(10, 10)) ##the tiling/no processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Set up ERA5 forcing:\n",
    "\n",
    "Here we assume the ERA5 dataset is stored somewhere on the system we are working on. \n",
    "\n",
    "Below is a table showing ERA5 characteristics and what needs to be done to sort it out.\n",
    "\n",
    "**Required ERA5 data**:\n",
    "\n",
    "Name | ERA5 filename | ERA5 variable name | Units\n",
    "---|---|---|---\n",
    "Surface Pressure | sp | sp | Pa \n",
    "Surface Temperature | 2t | t2m | K \n",
    "Meridional Wind | 10v | v10 | m/s \n",
    "Zonal Wind | 10u | u10 | m/s \n",
    "Specific Humidity | - | - | kg/kg, calculated from dewpoint temperature\n",
    "Dewpoint Temperature | 2d | d2m | K\n",
    "\n",
    "\n",
    "We calculate specific humidity $q$ from dewpoint temperature $T_d$ and surface pressure $P$ via saturation vapour pressure $P_v$.\n",
    "\n",
    "$$P_v = 10^{8.07131 - \\frac{1730.63}{233.426 + T}} \\frac{101325}{760} \\; \\textrm{[Pascal]} $$\n",
    "\n",
    "$$q = 0.001 \\times 0.622  \\frac{P_v}{P}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "expt.setup_era5(era_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Modify the default input directory to make a (hopefully) runnable configuration out of the box\n",
    "\n",
    "This step copies the default directory and modifies the `MOM_layout` files to match your experiment by inserting the right number of x, y points and CPU layout.\n",
    "\n",
    "To run MOM6 using the [payu infrastructure](https://github.com/payu-org/payu), provide the keyword argument `using_payu = True` to the `setup_run_directory` method and an example `config.yaml` file will be appear in the run directory. The `config.yaml` file needs to be modified manually to add the locations of executables, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.setup_run_directory(surface_forcing = \"era5\",using_payu = True, with_tides = False) \n",
    "# When tides files are found, run the previous command with parameter with_tides = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CrocoDash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
